{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.contrib.learn.python.learn.estimators.estimator import SKCompat\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.ops import array_ops as array_ops_\n",
    "learn = tf.contrib.learn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=[[1,6],[2,13],[3,23],[4,1],[5,4],[6,2],[7,7],[8,5],[9,1],[9,6]]\n",
    "train_y=[[13],[28],[49],[6],[13],[10],[21],[18],[11],[21]]\n",
    "for i in range(1000):\n",
    "    a=random.randint(0,20)\n",
    "    b=random.randint(0,20)\n",
    "    train_X.append([a,b])\n",
    "    train_y.append([a+2*b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f_num=2\n",
    "LEARNING_RATE_BASE = 0.0007\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "training_steps=100\n",
    "HIDDEN_SIZE=4\n",
    "OUTPUT_SIZE=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\",[None,f_num,])\n",
    "Y = tf.placeholder(\"float\", [None,OUTPUT_SIZE])\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "lay_1 = tf.Variable(tf.random_normal([f_num, HIDDEN_SIZE], mean=0.0, stddev=1.0))\n",
    "lay_2 = tf.Variable(tf.random_normal([HIDDEN_SIZE,OUTPUT_SIZE],mean=0.0,stddev=1.0))\n",
    "b_1 = tf.Variable(tf.zeros([HIDDEN_SIZE]),dtype=\"float32\",name=\"bias_1\")\n",
    "b_2 = tf.Variable(tf.zeros([OUTPUT_SIZE]),dtype=\"float32\",name=\"bias_2\")\n",
    "\n",
    "lay_out1 = tf.nn.sigmoid(tf.matmul(tf.reshape(X,[-1,f_num]), lay_1) + b_1)\n",
    "\n",
    "y_=tf.add(tf.matmul(tf.reshape(lay_out1,[-1,HIDDEN_SIZE]),lay_2),b_2)\n",
    "di=y_-Y\n",
    "loss_op = tf.reduce_mean(tf.square(y_ - Y))\n",
    "loss=tf.sqrt(loss_op)\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "        LEARNING_RATE_BASE,\n",
    "        global_step,\n",
    "        200, LEARNING_RATE_DECAY,\n",
    "        staircase=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train_op = optimizer.minimize(loss_op,global_step=global_step)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.1644\n",
      "31.1175\n",
      "30.4116\n",
      "29.6894\n",
      "29.0358\n",
      "28.307\n",
      "27.6599\n",
      "27.0262\n",
      "26.46\n",
      "25.9306\n",
      "25.4993\n",
      "24.858\n",
      "24.3657\n",
      "23.8928\n",
      "23.4419\n",
      "23.0688\n",
      "22.5973\n",
      "22.0889\n",
      "21.6888\n",
      "21.3707\n",
      "21.0113\n",
      "20.6951\n",
      "20.3836\n",
      "20.129\n",
      "19.9603\n",
      "19.74\n",
      "19.4221\n",
      "19.1215\n",
      "18.8548\n",
      "18.5559\n",
      "18.3253\n",
      "18.0914\n",
      "17.9166\n",
      "17.7142\n",
      "17.5249\n",
      "17.2885\n",
      "17.0654\n",
      "16.8341\n",
      "16.6426\n",
      "16.4584\n",
      "16.3476\n",
      "16.1131\n",
      "15.9705\n",
      "15.757\n",
      "15.5595\n",
      "15.4738\n",
      "15.2954\n",
      "15.1205\n",
      "14.8934\n",
      "14.8501\n",
      "14.6801\n",
      "14.4659\n",
      "14.2739\n",
      "14.1629\n",
      "14.0646\n",
      "14.1546\n",
      "13.7597\n",
      "13.7299\n",
      "13.4561\n",
      "13.3111\n",
      "13.3246\n",
      "13.2798\n",
      "13.0388\n",
      "13.0369\n",
      "13.0615\n",
      "12.9587\n",
      "12.5457\n",
      "12.9028\n",
      "12.5124\n",
      "12.4453\n",
      "12.3986\n",
      "12.3419\n",
      "12.1295\n",
      "12.1114\n",
      "11.9799\n",
      "11.8848\n",
      "12.0998\n",
      "11.6322\n",
      "11.739\n",
      "11.3456\n",
      "15.3478\n",
      "11.229\n",
      "11.7709\n",
      "11.2125\n",
      "11.3403\n",
      "11.3684\n",
      "10.9329\n",
      "10.7834\n",
      "11.3903\n",
      "10.8954\n",
      "10.5917\n",
      "10.7252\n",
      "10.4085\n",
      "10.4537\n",
      "10.362\n",
      "10.258\n",
      "10.0254\n",
      "9.99828\n",
      "10.1921\n",
      "11.07\n",
      "11.277\n",
      "9.9059\n",
      "9.85602\n",
      "9.82801\n",
      "9.83423\n",
      "9.56554\n",
      "9.65399\n",
      "9.71733\n",
      "9.85124\n",
      "12.017\n",
      "9.43561\n",
      "9.40876\n",
      "9.20356\n",
      "9.18959\n",
      "9.59692\n",
      "9.10974\n",
      "9.15698\n",
      "9.36865\n",
      "8.87036\n",
      "10.4628\n",
      "9.25993\n",
      "8.99215\n",
      "8.86546\n",
      "8.68348\n",
      "9.12527\n",
      "9.77108\n",
      "8.68825\n",
      "8.86097\n",
      "9.01933\n",
      "8.97657\n",
      "8.39328\n",
      "8.7745\n",
      "8.45313\n",
      "9.60572\n",
      "8.35169\n",
      "8.21702\n",
      "8.17524\n",
      "8.74555\n",
      "9.65411\n",
      "8.11967\n",
      "8.41293\n",
      "7.99834\n",
      "8.12305\n",
      "8.9646\n",
      "9.37064\n",
      "7.9707\n",
      "7.78643\n",
      "8.22986\n",
      "7.74164\n",
      "8.69513\n",
      "7.75771\n",
      "7.81044\n",
      "7.56911\n",
      "7.67342\n",
      "8.15154\n",
      "7.72507\n",
      "7.53888\n",
      "7.70812\n",
      "7.95828\n",
      "7.37437\n",
      "11.637\n",
      "7.64195\n",
      "7.38418\n",
      "7.4967\n",
      "7.38167\n",
      "7.30811\n",
      "8.82809\n",
      "7.24347\n",
      "7.19699\n",
      "9.9137\n",
      "7.42236\n",
      "7.45258\n",
      "7.1912\n",
      "7.05688\n",
      "7.12411\n",
      "7.08975\n",
      "7.27954\n",
      "6.93132\n",
      "7.45216\n",
      "6.97719\n",
      "6.92476\n",
      "8.22182\n",
      "7.19635\n",
      "6.94683\n",
      "7.0451\n",
      "6.80491\n",
      "6.72529\n",
      "7.0036\n",
      "6.77317\n",
      "6.68473\n",
      "6.66092\n",
      "6.62335\n",
      "6.66023\n",
      "6.99381\n",
      "6.59148\n",
      "6.99388\n",
      "9.06979\n",
      "6.78984\n",
      "7.11511\n",
      "6.42823\n",
      "6.62266\n",
      "6.81173\n",
      "6.43444\n",
      "6.4264\n",
      "11.175\n",
      "6.51416\n",
      "9.24857\n",
      "6.95524\n",
      "8.99806\n",
      "6.37926\n",
      "6.51258\n",
      "7.13776\n",
      "6.69748\n",
      "6.2946\n",
      "7.15913\n",
      "6.23814\n",
      "6.31898\n",
      "6.24378\n",
      "6.21641\n",
      "6.48096\n",
      "6.23457\n",
      "6.12212\n",
      "6.11916\n",
      "6.25753\n",
      "6.11118\n",
      "6.17216\n",
      "8.2326\n",
      "7.41511\n",
      "11.0139\n",
      "8.0526\n",
      "6.07844\n",
      "6.07413\n",
      "10.3147\n",
      "6.20004\n",
      "7.29487\n",
      "7.42727\n",
      "6.01813\n",
      "6.0911\n",
      "5.97406\n",
      "6.51879\n",
      "6.11416\n",
      "5.89957\n",
      "5.88653\n",
      "9.08759\n",
      "6.66448\n",
      "5.90408\n",
      "6.02855\n",
      "6.94886\n",
      "6.38812\n",
      "6.62657\n",
      "6.13678\n",
      "5.84173\n",
      "5.77543\n",
      "6.44811\n",
      "5.79625\n",
      "5.76892\n",
      "5.92673\n",
      "5.91255\n",
      "5.73859\n",
      "10.0531\n",
      "10.9648\n",
      "6.30659\n",
      "6.03533\n",
      "5.87981\n",
      "5.68843\n",
      "6.42709\n",
      "5.77147\n",
      "5.7506\n",
      "5.79097\n",
      "7.99222\n",
      "6.00289\n",
      "5.63588\n",
      "5.69808\n",
      "8.37989\n",
      "5.74565\n",
      "5.91457\n",
      "5.57945\n",
      "5.96945\n",
      "5.55427\n",
      "5.53473\n",
      "5.49077\n",
      "5.47124\n",
      "5.55547\n",
      "6.24739\n",
      "5.49664\n",
      "5.57944\n",
      "5.57895\n",
      "5.46887\n",
      "5.84108\n",
      "5.72959\n",
      "6.1536\n",
      "5.4842\n",
      "5.54299\n",
      "5.54509\n",
      "5.36603\n",
      "5.32463\n",
      "8.64052\n",
      "6.08127\n",
      "7.15913\n",
      "5.35873\n",
      "5.728\n",
      "5.56538\n",
      "5.38201\n",
      "5.42219\n",
      "5.44717\n",
      "5.57322\n",
      "6.24449\n",
      "5.66081\n",
      "5.69743\n",
      "5.31559\n",
      "5.51052\n",
      "7.198\n",
      "5.55676\n",
      "12.1197\n",
      "5.2576\n",
      "5.37654\n",
      "5.20069\n",
      "5.75269\n",
      "5.25176\n",
      "5.24197\n",
      "5.16956\n",
      "5.2631\n",
      "5.19551\n",
      "5.23575\n",
      "5.15034\n",
      "6.16332\n",
      "5.28339\n",
      "5.54673\n",
      "9.59816\n",
      "5.21879\n",
      "5.19669\n",
      "5.35585\n",
      "7.56867\n",
      "5.30192\n",
      "5.16986\n",
      "5.27166\n",
      "6.50372\n",
      "5.22589\n",
      "5.16713\n",
      "6.86632\n",
      "5.12354\n",
      "5.05945\n",
      "5.33997\n",
      "7.99177\n",
      "5.32351\n",
      "5.0993\n",
      "5.05445\n",
      "5.12801\n",
      "5.52955\n",
      "5.59456\n",
      "5.86632\n",
      "5.14965\n",
      "5.39922\n",
      "5.41621\n",
      "5.08618\n",
      "5.95713\n",
      "5.43293\n",
      "4.95499\n",
      "4.97231\n",
      "5.61219\n",
      "5.83198\n",
      "5.14183\n",
      "5.36934\n",
      "4.9888\n",
      "7.73222\n",
      "5.29102\n",
      "5.17468\n",
      "5.43571\n",
      "4.97939\n",
      "5.73728\n",
      "4.95086\n",
      "4.90512\n",
      "5.0324\n",
      "5.2443\n",
      "5.04246\n",
      "5.68271\n",
      "5.76399\n",
      "5.21836\n",
      "5.1562\n",
      "4.90922\n",
      "5.16031\n",
      "6.64349\n",
      "5.0962\n",
      "4.82232\n",
      "4.86284\n",
      "4.92451\n",
      "5.01175\n",
      "4.87574\n",
      "5.44866\n",
      "6.27094\n",
      "5.12006\n",
      "5.01517\n",
      "5.06449\n",
      "4.80409\n",
      "4.7812\n",
      "10.364\n",
      "4.93523\n",
      "4.84901\n",
      "4.85424\n",
      "5.07762\n",
      "5.77208\n",
      "4.88431\n",
      "4.84069\n",
      "8.9656\n",
      "5.28537\n",
      "6.82986\n",
      "5.07758\n",
      "6.58972\n",
      "5.06274\n",
      "5.20863\n",
      "5.4695\n",
      "4.82472\n",
      "4.84539\n",
      "4.80345\n",
      "4.84751\n",
      "4.8971\n",
      "4.67268\n",
      "5.49015\n",
      "4.86322\n",
      "4.80126\n",
      "4.74698\n",
      "4.67161\n",
      "4.97373\n",
      "4.70069\n",
      "4.72872\n",
      "7.65623\n",
      "6.49405\n",
      "8.77704\n",
      "9.02073\n",
      "4.67794\n",
      "4.91322\n",
      "5.03281\n",
      "4.92109\n",
      "4.8644\n",
      "4.78832\n",
      "4.75405\n",
      "4.67378\n",
      "4.91936\n",
      "7.80283\n",
      "4.89367\n",
      "4.84657\n",
      "4.61425\n",
      "6.46398\n",
      "5.30174\n",
      "4.79064\n",
      "4.71161\n",
      "6.27701\n",
      "5.20659\n",
      "5.20709\n",
      "5.01425\n",
      "4.66444\n",
      "5.04355\n",
      "5.11972\n",
      "4.73329\n",
      "4.70599\n",
      "4.79383\n",
      "5.38761\n",
      "4.6465\n",
      "9.90057\n",
      "4.95635\n",
      "4.75911\n",
      "4.57082\n",
      "4.4953\n",
      "6.24331\n",
      "5.8721\n",
      "4.90326\n",
      "5.53524\n",
      "5.15301\n",
      "10.6208\n",
      "4.74719\n",
      "4.96873\n",
      "5.025\n",
      "4.48006\n",
      "4.76949\n",
      "5.58896\n",
      "5.16718\n",
      "4.6401\n",
      "4.56905\n",
      "4.54281\n",
      "4.6495\n",
      "4.77656\n",
      "4.60796\n",
      "4.72411\n",
      "4.80054\n",
      "5.46843\n",
      "4.69973\n",
      "5.10474\n",
      "5.40584\n",
      "4.87913\n",
      "4.70408\n",
      "4.98801\n",
      "4.43622\n",
      "9.53828\n",
      "4.71199\n",
      "4.45837\n",
      "8.4768\n",
      "4.46741\n",
      "5.05069\n",
      "4.81486\n",
      "4.81612\n",
      "4.46536\n",
      "4.58827\n",
      "5.41836\n",
      "5.20121\n",
      "6.20551\n",
      "8.28753\n",
      "4.99799\n",
      "5.49752\n",
      "4.551\n",
      "5.17705\n",
      "5.71453\n",
      "4.73004\n",
      "8.18744\n",
      "4.39413\n",
      "4.42085\n",
      "4.37095\n",
      "6.63269\n",
      "4.54439\n",
      "4.52614\n",
      "6.45175\n",
      "4.73636\n",
      "4.38985\n",
      "4.4563\n",
      "4.33872\n",
      "4.36224\n",
      "4.4206\n",
      "4.66955\n",
      "6.123\n",
      "4.6078\n",
      "4.62798\n",
      "4.6307\n",
      "7.53939\n",
      "6.13369\n",
      "4.82256\n",
      "4.82717\n",
      "5.80084\n",
      "4.40775\n",
      "4.34009\n",
      "4.35106\n",
      "4.32161\n",
      "4.33972\n",
      "4.55759\n",
      "4.84377\n",
      "5.06146\n",
      "4.3903\n",
      "4.29117\n",
      "5.478\n",
      "5.61539\n",
      "4.32343\n",
      "4.89612\n",
      "4.59441\n",
      "5.32666\n",
      "4.46703\n",
      "4.31236\n",
      "4.5233\n",
      "4.78648\n",
      "4.33167\n",
      "4.70942\n",
      "4.2847\n",
      "5.03263\n",
      "4.28299\n",
      "4.29393\n",
      "4.43527\n",
      "4.96281\n",
      "4.92039\n",
      "5.03745\n",
      "5.13475\n",
      "4.60257\n",
      "5.0427\n",
      "4.35457\n",
      "4.26311\n",
      "4.348\n",
      "5.41247\n",
      "4.51657\n",
      "4.8147\n",
      "5.62697\n",
      "4.43963\n",
      "4.66311\n",
      "4.24304\n",
      "4.35827\n",
      "5.48926\n",
      "4.62616\n",
      "4.29366\n",
      "4.58989\n",
      "4.46081\n",
      "4.46595\n",
      "5.31422\n",
      "5.25723\n",
      "7.15584\n",
      "4.4551\n",
      "4.50983\n",
      "4.61426\n",
      "4.38072\n",
      "4.30405\n",
      "7.47116\n",
      "4.33875\n",
      "4.42477\n",
      "4.35503\n",
      "5.01141\n",
      "4.8218\n",
      "[array([[ 14.98618507],\n",
      "       [ 15.84608269],\n",
      "       [ 20.59815598],\n",
      "       [ 37.17794037]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    batch=0\n",
    "    for step in range(6000):\n",
    "        sess.run(train_op, feed_dict={X: train_X[batch:batch+5], Y: train_y[batch:batch+5]})\n",
    "        batch=batch+1\n",
    "        if batch + 5 == 1000:\n",
    "            batch=0\n",
    "        if step % 10 == 0:\n",
    "            pass\n",
    "                #print(sess.run(loss,feed_dict={X:np.array(date_list[0].iloc[5],dtype=np.float32), Y: np.array(sale_list[0].iloc[-1],dtype=np.float32)}))\n",
    "            print(sess.run(loss,feed_dict={X:train_X, Y: train_y}))\n",
    "    print(sess.run([y_],feed_dict={X:[[5,5],[4,6],[17,2],[20,8]]}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
