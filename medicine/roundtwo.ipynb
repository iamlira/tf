{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold, cross_val_score\n",
    "from datetime import datetime\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder, OneHotEncoder, minmax_scale, scale, LabelBinarizer\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold,RFE, f_regression\n",
    "#from minepy import MINE\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "color = sns.color_palette()\n",
    "from mlxtend.regressor import StackingRegressor, StackingCVRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample=pd.read_csv('/home/dope/data_set/medicine/f_sample_20180204.csv')\n",
    "train=pd.read_csv('/home/dope/data_set/medicine/f_train_20180204.csv',encoding='gbk')\n",
    "test=pd.read_csv('/home/dope/data_set/medicine/f_test_a_20180204.csv',encoding='gbk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['id', 'SNP1', 'SNP2', 'SNP3', 'SNP4', 'SNP5', 'SNP6', 'SNP7',\n",
       "       'SNP8', 'SNP9', 'SNP10', 'SNP11', 'SNP12', 'SNP13', 'SNP14',\n",
       "       'SNP15', 'SNP16', 'SNP17', 'SNP18', 'SNP19', 'SNP20', 'SNP21',\n",
       "       'SNP22', 'SNP23', 'RBP4', '年龄', '孕次', '产次', '身高', '孕前体重', 'BMI分类',\n",
       "       '孕前BMI', '收缩压', '舒张压', '分娩时', '糖筛孕周', 'VAR00007', 'wbc', 'ALT',\n",
       "       'AST', 'Cr', 'BUN', 'CHO', 'TG', 'HDLC', 'LDLC', 'ApoA1', 'ApoB',\n",
       "       'Lpa', 'hsCRP', 'SNP24', 'SNP25', 'SNP26', 'SNP27', 'SNP28',\n",
       "       'SNP29', 'SNP30', 'SNP31', 'SNP32', 'SNP33', 'SNP34', 'SNP35',\n",
       "       'SNP36', 'SNP37', 'SNP38', 'DM家族史', 'SNP39', 'SNP40', 'SNP41',\n",
       "       'SNP42', 'SNP43', 'SNP44', 'SNP45', 'SNP46', 'SNP47', 'SNP48',\n",
       "       'SNP49', 'SNP50', 'SNP51', 'SNP52', 'SNP53', 'SNP54', 'SNP55',\n",
       "       'ACEID', 'label'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义处理数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_dropna(df,target_data):\n",
    "    des=df.describe().ix[0]\n",
    "    index=0\n",
    "    for col in df.columns.values:\n",
    "        if des[index]/len(df[col]) <= 0.81:\n",
    "            df.drop(col,axis=1,inplace=True)\n",
    "            target_data.drop(col,axis=1,inplace=True)\n",
    "        index=index+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_fillna(df):\n",
    "    df.fillna(df.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_norm(df):\n",
    "    for col in df.columns.values:\n",
    "        if df[col].mean() > 3 :\n",
    "            df[col] = df[col] / (df[col].mean() / 3)\n",
    "            #for index in range(len(df[col])):\n",
    "            #   df[col].ix[index] = df[col].ix[index] / ( df[col].mean() / 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(df):\n",
    "    for j in [col for col in df]:\n",
    "        t_max=df[j].max()\n",
    "        t_min=df[j].min()\n",
    "        down = 1 if (t_max-t_min == 0) else (t_max-t_min)\n",
    "        df[j]=(df[j]-t_min) / down * 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train=train['label']\n",
    "train.drop('label',axis=1,inplace=True)\n",
    "train.drop('id',axis=1,inplace=True)\n",
    "test.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dope/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "my_dropna(train,test)\n",
    "my_fillna(train)\n",
    "my_fillna(test)\n",
    "#my_norm(train)\n",
    "train=preprocessing.scale(train)\n",
    "test= preprocessing.scale(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "for index , value in enumerate(label_train):\n",
    "    y_train.append([1,0]) if value == 1 else y_train.append([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -7.07460600e-02,  -1.11069483e+00,   1.04179778e-15, ...,\n",
       "          3.08135826e-01,  -7.29479467e-01,  -4.86389039e-01],\n",
       "       [  1.34417514e+00,   2.95247994e-01,   4.77597648e-01, ...,\n",
       "         -1.18093435e+00,   1.02233619e+00,  -4.86389039e-01],\n",
       "       [ -1.48566726e+00,  -1.11069483e+00,   4.77597648e-01, ...,\n",
       "          3.08135826e-01,  -7.29479467e-01,  -4.86389039e-01],\n",
       "       ..., \n",
       "       [  1.34417514e+00,   2.95247994e-01,   4.77597648e-01, ...,\n",
       "          3.08135826e-01,  -7.29479467e-01,   1.75992942e+00],\n",
       "       [  1.34417514e+00,  -1.11069483e+00,  -1.86832241e+00, ...,\n",
       "          3.08135826e-01,  -7.29479467e-01,  -4.86389039e-01],\n",
       "       [  1.34417514e+00,   2.95247994e-01,   4.77597648e-01, ...,\n",
       "          3.08135826e-01,  -7.29479467e-01,   1.75992942e+00]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for col in train.columns.values:\n",
    "    print(col + \":\" + str(train[col].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_WIDTH=8\n",
    "CONV_WIDTH=3\n",
    "TRAINNING_STEP=1000\n",
    "IN_CHANNEL_SIZE=1\n",
    "OUT_CHANNEL_SIZE=32\n",
    "OUT_CHANNEL_1_SIZE=64\n",
    "OUT_CHANNEL_2_SIZE=128\n",
    "OUTPUT_SIZE=2\n",
    "\n",
    "LEARNING_RATE_BASE = 0.08\n",
    "LEARNING_RATE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.3)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    # Must have strides[0] = strides[3] = 1\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def get_train_data(data,times):\n",
    "    return_data=[]\n",
    "    for i in range(10):\n",
    "        return_data.append(np.array(train.ix[times*10+i]).reshape([8,8,1]))\n",
    "    return return_data\n",
    "def get_train_y(label,times):\n",
    "    return_y=[]\n",
    "    for i in range(10):\n",
    "        return_y.append(y_train[times*10+i])\n",
    "    return return_y\n",
    "\n",
    "def get_test_data(data):\n",
    "    return_data=[]\n",
    "    for i in range(200):\n",
    "        return_data.append(np.array(train.ix[800+i]).reshape([8,8,1]))\n",
    "    return return_data\n",
    "\n",
    "def get_test_y(data):\n",
    "    return_y=[]\n",
    "    for i in range(200):\n",
    "        return_y.append(y_train[800+i])\n",
    "    return return_y\n",
    "\n",
    "def get_train_data_trainfuc(data,times):\n",
    "    return_data=[]\n",
    "    for i in range(10):\n",
    "        return_data.append(np.array(data[times*10+i]).reshape([8,8,1]))\n",
    "    return return_data\n",
    "def get_train_y_trainfuc(label,times):\n",
    "    return_y=[]\n",
    "    for i in range(10):\n",
    "        return_y.append(label[times*10+i])\n",
    "    return return_y\n",
    "\n",
    "def get_test_data_trainfuc(data):\n",
    "    return_data=[]\n",
    "    for i in range(200):\n",
    "        return_data.append(np.array(data[i]).reshape([8,8,1]))\n",
    "    return return_data\n",
    "\n",
    "def get_test_y_trainfuc(data):\n",
    "    return_y=[]\n",
    "    for i in range(200):\n",
    "        return_y.append(data[i])\n",
    "    return return_y\n",
    "\n",
    "def get_target_data_trainfuc(data):\n",
    "    return_x=[]\n",
    "    for i in range(len(data)):\n",
    "        return_x.append(np.array(data[i]).reshape([8,8,1]))\n",
    "    return return_x\n",
    "\n",
    "def cal_acc():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"cnn\",reuse=tf.AUTO_REUSE):\n",
    "    train_X = tf.placeholder(tf.float32,[None,INPUT_WIDTH,INPUT_WIDTH,1])\n",
    "    train_y = tf.placeholder(tf.float32,[None,OUTPUT_SIZE])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # conv1 layer\n",
    "    W_conv1 = weight_variable([CONV_WIDTH,CONV_WIDTH,IN_CHANNEL_SIZE,OUT_CHANNEL_SIZE])\n",
    "    b_conv1 = bias_variable([OUT_CHANNEL_SIZE])\n",
    "    h_conv1 = tf.nn.tanh(conv2d(train_X,W_conv1)+b_conv1) #output size is 8x8x2\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #output size is 4x4xOUT_CHANNEL_SIZE\n",
    "    \n",
    "    #conv2 layer\n",
    "    W_conv2 = weight_variable([CONV_WIDTH,CONV_WIDTH,OUT_CHANNEL_SIZE,OUT_CHANNEL_1_SIZE])\n",
    "    b_conv2 = bias_variable([OUT_CHANNEL_1_SIZE])\n",
    "    h_conv2 = tf.nn.tanh(conv2d(h_pool1,W_conv2)+b_conv2)#output size is 4x4xOUT_CHANNEL_1_SIZE\n",
    "    h_pool2 = max_pool_2x2(h_conv2) #output size is 2x2xOUT_CHANNEL_1_SIZE\n",
    "    \n",
    "    #conv3 layer\n",
    "    #W_conv3 = weight_variable([CONV_WIDTH,CONV_WIDTH,OUT_CHANNEL_1_SIZE,OUT_CHANNEL_2_SIZE])\n",
    "    #b_conv3 = bias_variable([OUT_CHANNEL_2_SIZE])\n",
    "    #h_conv3 = tf.nn.tanh(conv2d(h_pool2,W_conv3)+b_conv3) #output size is 4x4xOUT_CHANNEL_2_SIZE\n",
    "    #h_pool3 = max_pool_2x2(h_conv3) #output size is 2*2*OUT_CHANNEL_2_SIZE\n",
    "    \n",
    "    #func1 layer\n",
    "    W_fc1 = weight_variable([4*4*OUT_CHANNEL_1_SIZE,6])\n",
    "    b_fc1 = bias_variable([6])\n",
    "    h_pool1_flat = tf.reshape(h_conv2,[-1,4*4*OUT_CHANNEL_1_SIZE])\n",
    "    h_fc1 = tf.nn.tanh(tf.matmul(h_pool1_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    #func2 layer\n",
    "    W_fc2 = weight_variable([6,2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    prediction = tf.nn.softmax(tf.matmul(h_fc1,W_fc2) + b_fc2)\n",
    "    \n",
    "    #ops\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(train_y * tf.log(prediction),reduction_indices=[1]))\n",
    "    #loss_op = tf.reduce_mean(tf.square(train_y - prediction))\n",
    "    \n",
    "    #other param\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            200, LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy,global_step=global_step)\n",
    "    \n",
    "    #accuracy\n",
    "    y_true = tf.argmin(train_y,1)\n",
    "    y_pred= tf.argmin(prediction,1)\n",
    "    correct_prediction = tf.equal(tf.argmin(prediction,1), tf.argmin(train_y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for j in range(86):\n",
    "        for i in range(80):\n",
    "            _data=get_train_data(train,i)\n",
    "            _label=get_train_y(y_train,i)\n",
    "            sess.run(train_op,feed_dict={train_X:_data , train_y:_label , keep_prob:0.3})\n",
    "        print(j,sess.run(cross_entropy,feed_dict={train_X:[_data[0]] , train_y:[_label[0]] , keep_prob:0.3}))\n",
    "    #sess.run(train_op,feed_dict={train_X: [np.array(train.ix[0]).reshape([8,8,1]) , np.array(train.ix[1]).reshape([8,8,1])] , train_y:[y_train[0],y_train[1]] , keep_prob:0.3})\n",
    "    test_data = get_test_data(train)\n",
    "    test_y = get_test_y(y_train)\n",
    "    print(sess.run(prediction,feed_dict={train_X:_data , train_y:_label , keep_prob:0.3}))\n",
    "    print(sess.run(accuracy,feed_dict={train_X:test_data , train_y:test_y , keep_prob:0.3}))\n",
    "        \n",
    "    true_y , pred_y = sess.run([y_true , y_pred],feed_dict={train_X:test_data , train_y:test_y , keep_prob:0.3})\n",
    "    p = precision_score(true_y,pred_y,average='binary')\n",
    "    r = recall_score(true_y,pred_y,average='binary')\n",
    "    f1score = f1_score(true_y,pred_y,average='binary')\n",
    "    print(p,r,f1score)\n",
    "    #print(W_conv1.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train_func(train_data,train_label,test_data,test_label):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for j in range(86):\n",
    "            for i in range(int(len(train_data)/10)):\n",
    "                _data=get_train_data_trainfuc(train_data,i)\n",
    "                _label=get_train_y_trainfuc(train_label,i)\n",
    "                sess.run(train_op,feed_dict={train_X:_data , train_y:_label , keep_prob:0.3})\n",
    "            #print(j,sess.run(cross_entropy,feed_dict={train_X:[_data[0]] , train_y:[_label[0]] , keep_prob:0.3}))\n",
    "        #sess.run(train_op,feed_dict={train_X: [np.array(train.ix[0]).reshape([8,8,1]) , np.array(train.ix[1]).reshape([8,8,1])] , train_y:[y_train[0],y_train[1]] , keep_prob:0.3})\n",
    "        test_data_fuc = get_test_data_trainfuc(test_data)\n",
    "        test_y_fuc = get_test_y_trainfuc(test_label)\n",
    "        print(sess.run(prediction,feed_dict={train_X:_data , train_y:_label , keep_prob:0.3}))\n",
    "        print(sess.run(accuracy,feed_dict={train_X:test_data_fuc , train_y:test_y_fuc , keep_prob:0.3}))\n",
    "            \n",
    "        true_y , pred_y = sess.run([y_true , y_pred],feed_dict={train_X:test_data_fuc , train_y:test_y_fuc })\n",
    "        p = precision_score(true_y,pred_y,average='binary')\n",
    "        r = recall_score(true_y,pred_y,average='binary')\n",
    "        f1score = f1_score(true_y,pred_y,average='binary')\n",
    "        print(p,r,f1score)\n",
    "        \n",
    "        gene_target=get_target_data_trainfuc(test)\n",
    "        target = sess.run([y_pred],feed_dict={train_X:gene_target})\n",
    "        target=np.array(target).reshape((200))\n",
    "        target_csv = pd.DataFrame(columns=['result'],data=target)\n",
    "        target_csv.to_csv('./submission/submission'+str(f1score)+'.csv',index=False)\n",
    "        #print(W_conv1.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((train[0:0],train[200:])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.98813272e-01   1.18679611e-03]\n",
      " [  9.99981403e-01   1.86347679e-05]\n",
      " [  9.48484898e-01   5.15151508e-02]\n",
      " [  2.85156100e-04   9.99714792e-01]\n",
      " [  9.99977827e-01   2.21496939e-05]\n",
      " [  9.99881625e-01   1.18409349e-04]\n",
      " [  8.44564319e-01   1.55435696e-01]\n",
      " [  9.99983311e-01   1.66679110e-05]\n",
      " [  9.90584910e-01   9.41510592e-03]\n",
      " [  9.97151196e-01   2.84885964e-03]]\n",
      "0.65\n",
      "0.638554216867 0.569892473118 0.602272727273\n",
      "[[  9.97906327e-01   2.09368905e-03]\n",
      " [  9.97593105e-01   2.40692217e-03]\n",
      " [  9.96760666e-01   3.23941512e-03]\n",
      " [  8.14419065e-04   9.99185622e-01]\n",
      " [  9.97911394e-01   2.08861334e-03]\n",
      " [  9.97377872e-01   2.62215175e-03]\n",
      " [  9.99668121e-01   3.31861200e-04]\n",
      " [  9.97910559e-01   2.08942220e-03]\n",
      " [  9.97324467e-01   2.67547066e-03]\n",
      " [  9.50540125e-01   4.94598709e-02]]\n",
      "0.645\n",
      "0.730769230769 0.532710280374 0.616216216216\n",
      "[[  9.98396814e-01   1.60322350e-03]\n",
      " [  9.98398125e-01   1.60181429e-03]\n",
      " [  9.11517620e-01   8.84823948e-02]\n",
      " [  6.36430748e-04   9.99363601e-01]\n",
      " [  9.98397887e-01   1.60212792e-03]\n",
      " [  9.98401940e-01   1.59801438e-03]\n",
      " [  8.94574642e-01   1.05425373e-01]\n",
      " [  9.99759853e-01   2.40200709e-04]\n",
      " [  9.96867120e-01   3.13281594e-03]\n",
      " [  7.14330152e-02   9.28566933e-01]]\n",
      "0.65\n",
      "0.584905660377 0.704545454545 0.639175257732\n",
      "[[  9.98118520e-01   1.88146043e-03]\n",
      " [  9.99964118e-01   3.59290461e-05]\n",
      " [  8.71441483e-01   1.28558517e-01]\n",
      " [  1.67466886e-02   9.83253300e-01]\n",
      " [  9.99977231e-01   2.27376768e-05]\n",
      " [  9.99980688e-01   1.93556007e-05]\n",
      " [  9.81357217e-01   1.86427422e-02]\n",
      " [  9.99900818e-01   9.91512134e-05]\n",
      " [  8.82847726e-01   1.17152289e-01]\n",
      " [  9.99982595e-01   1.74319630e-05]]\n",
      "0.66\n",
      "0.636363636364 0.663157894737 0.649484536082\n",
      "[[  1.26308771e-02   9.87369120e-01]\n",
      " [  5.61992172e-03   9.94380057e-01]\n",
      " [  1.90018138e-04   9.99810040e-01]\n",
      " [  1.26252575e-02   9.87374663e-01]\n",
      " [  9.93734896e-01   6.26510661e-03]\n",
      " [  9.93734896e-01   6.26510335e-03]\n",
      " [  9.91573513e-01   8.42649676e-03]\n",
      " [  1.69086532e-04   9.99830842e-01]\n",
      " [  1.26223927e-02   9.87377584e-01]\n",
      " [  9.93456185e-01   6.54378906e-03]]\n",
      "0.635\n",
      "0.561904761905 0.686046511628 0.61780104712\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    test_data=train[index*200:index*200+200]\n",
    "    test_label=y_train[index*200:index*200+200]\n",
    "    train_data=np.concatenate((train[0:index*200],train[index*200+200:]))\n",
    "    train_label=y_train[0:index*200]+y_train[index*200+200:]\n",
    "    train_func(train_data,train_label,test_data,test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
