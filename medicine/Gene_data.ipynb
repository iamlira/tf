{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dope/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/dope/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import KFold, StratifiedKFold, cross_val_score\n",
    "from datetime import datetime\n",
    "#from catboost import CatBoostRegressor\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder, OneHotEncoder, minmax_scale, scale, LabelBinarizer\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_selection import SelectFromModel, VarianceThreshold,RFE, f_regression\n",
    "#from minepy import MINE\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "color = sns.color_palette()\n",
    "from mlxtend.regressor import StackingRegressor, StackingCVRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gene=pd.read_csv('/home/dope/data_set/medicine/Gene_data.csv')\n",
    "train=pd.read_csv('/home/dope/data_set/medicine/f_train_20180204.csv',encoding='gbk')\n",
    "label_train=train['label']\n",
    "y_train=[]\n",
    "for index , value in enumerate(label_train):\n",
    "    y_train.append([1,0]) if value == 1 else y_train.append([0,1])\n",
    "gene_scaled=preprocessing.scale(gene)\n",
    "gene_test=gene_scaled[1000:]\n",
    "gene_train=gene_scaled[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gene_scaled[0].reshape(702,9,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INPUT_WIDTH=702\n",
    "INPUT_HEIGHT=9\n",
    "CONV_WIDTH=3\n",
    "TRAINNING_STEP=1000\n",
    "IN_CHANNEL_SIZE=1\n",
    "OUT_CHANNEL_SIZE=32\n",
    "OUT_CHANNEL_1_SIZE=64\n",
    "OUT_CHANNEL_2_SIZE=128\n",
    "OUTPUT_SIZE=2\n",
    "\n",
    "LEARNING_RATE_BASE = 0.025\n",
    "LEARNING_RATE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.11)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    # Must have strides[0] = strides[3] = 1\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # stride [1, x_movement, y_movement, 1]\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def get_train_data(data,times):\n",
    "    return_data=[]\n",
    "    for i in range(10):\n",
    "        return_data.append(np.array(train.ix[times*10+i]).reshape([INPUT_WIDTH,INPUT_HEIGHT,1]))\n",
    "    return return_data\n",
    "def get_train_y(label,times):\n",
    "    return_y=[]\n",
    "    for i in range(10):\n",
    "        return_y.append(y_train[times*10+i])\n",
    "    return return_y\n",
    "\n",
    "def get_test_data(data):\n",
    "    return_data=[]\n",
    "    for i in range(200):\n",
    "        return_data.append(np.array(train.ix[800+i]).reshape([INPUT_WIDTH,INPUT_HEIGHT,1]))\n",
    "    return return_data\n",
    "\n",
    "def get_test_y(data):\n",
    "    return_y=[]\n",
    "    for i in range(200):\n",
    "        return_y.append(y_train[800+i])\n",
    "    return return_y\n",
    "\n",
    "def get_train_data_trainfuc(data,times):\n",
    "    return_data=[]\n",
    "    for i in range(10):\n",
    "        return_data.append(np.array(data[times*10+i]).reshape([INPUT_WIDTH,INPUT_HEIGHT,1]))\n",
    "    return return_data\n",
    "def get_train_y_trainfuc(label,times):\n",
    "    return_y=[]\n",
    "    for i in range(10):\n",
    "        return_y.append(label[times*10+i])\n",
    "    return return_y\n",
    "\n",
    "def get_test_data_trainfuc(data):\n",
    "    return_data=[]\n",
    "    for i in range(200):\n",
    "        return_data.append(np.array(data[i]).reshape([INPUT_WIDTH,INPUT_HEIGHT,1]))\n",
    "    return return_data\n",
    "\n",
    "def get_test_y_trainfuc(data):\n",
    "    return_y=[]\n",
    "    for i in range(200):\n",
    "        return_y.append(data[i])\n",
    "    return return_y\n",
    "\n",
    "def get_target_data_trainfuc(data):\n",
    "    return_x=[]\n",
    "    for i in range(len(data)):\n",
    "        return_x.append(np.array(data[i]).reshape([INPUT_WIDTH,INPUT_HEIGHT,1]))\n",
    "    return return_x\n",
    "\n",
    "def cal_acc():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"cnn\",reuse=tf.AUTO_REUSE):\n",
    "    train_X = tf.placeholder(tf.float32,[None,INPUT_WIDTH,INPUT_HEIGHT,1])\n",
    "    train_y = tf.placeholder(tf.float32,[None,OUTPUT_SIZE])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # conv1 layer\n",
    "    W_conv1 = weight_variable([CONV_WIDTH,CONV_WIDTH,IN_CHANNEL_SIZE,OUT_CHANNEL_SIZE])\n",
    "    b_conv1 = bias_variable([OUT_CHANNEL_SIZE])\n",
    "    h_conv1 = tf.nn.tanh(conv2d(train_X,W_conv1)+b_conv1) #output size is 702x9xOUT_CHANNEL_SIZE\n",
    "    h_pool1 = max_pool_2x2(h_conv1) #output size is 351x5xOUT_CHANNEL_SIZE\n",
    "    \n",
    "    #conv2 layer\n",
    "    W_conv2 = weight_variable([CONV_WIDTH,CONV_WIDTH,OUT_CHANNEL_SIZE,OUT_CHANNEL_1_SIZE])\n",
    "    b_conv2 = bias_variable([OUT_CHANNEL_1_SIZE])\n",
    "    h_conv2 = tf.nn.tanh(conv2d(h_pool1,W_conv2)+b_conv2)#output size is 351x5xOUT_CHANNEL_1_SIZE\n",
    "    h_pool2 = max_pool_2x2(h_conv2) #output size is  x xOUT_CHANNEL_1_SIZE\n",
    "    \n",
    "    #conv3 layer\n",
    "    #W_conv3 = weight_variable([CONV_WIDTH,CONV_WIDTH,OUT_CHANNEL_1_SIZE,OUT_CHANNEL_2_SIZE])\n",
    "    #b_conv3 = bias_variable([OUT_CHANNEL_2_SIZE])\n",
    "    #h_conv3 = tf.nn.tanh(conv2d(h_pool2,W_conv3)+b_conv3) #output size is 4x4xOUT_CHANNEL_2_SIZE\n",
    "    #h_pool3 = max_pool_2x2(h_conv3) #output size is 2*2*OUT_CHANNEL_2_SIZE\n",
    "    \n",
    "    #func1 layer\n",
    "    W_fc1 = weight_variable([351*5*OUT_CHANNEL_1_SIZE,6])\n",
    "    b_fc1 = bias_variable([6])\n",
    "    h_pool1_flat = tf.reshape(h_conv2,[-1,351*5*OUT_CHANNEL_1_SIZE])\n",
    "    h_fc1 = tf.nn.tanh(tf.matmul(h_pool1_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "    \n",
    "    #func2 layer\n",
    "    W_fc2 = weight_variable([6,2])\n",
    "    b_fc2 = bias_variable([2])\n",
    "    prediction = tf.nn.softmax(tf.matmul(h_fc1,W_fc2) + b_fc2)\n",
    "    \n",
    "    #ops\n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(train_y * tf.log(prediction),reduction_indices=[1]))\n",
    "    #loss_op = tf.reduce_mean(tf.square(train_y - prediction))\n",
    "    \n",
    "    #other param\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            200, LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(cross_entropy,global_step=global_step)\n",
    "    \n",
    "    #accuracy\n",
    "    y_true = tf.argmin(train_y,1)\n",
    "    y_pred= tf.argmin(prediction,1)\n",
    "    correct_prediction = tf.equal(tf.argmin(prediction,1), tf.argmin(train_y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_csv=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_func(train_data,train_label,test_data,test_label):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for j in range(86):\n",
    "            for i in range(int(len(train_data)/10)):\n",
    "                _data=get_train_data_trainfuc(train_data,i)\n",
    "                _label=get_train_y_trainfuc(train_label,i)\n",
    "                sess.run(train_op,feed_dict={train_X:_data , train_y:_label , keep_prob:0.3})\n",
    "            #print(j,sess.run(cross_entropy,feed_dict={train_X:[_data[0]] , train_y:[_label[0]] , keep_prob:0.3}))\n",
    "        #sess.run(train_op,feed_dict={train_X: [np.array(train.ix[0]).reshape([8,8,1]) , np.array(train.ix[1]).reshape([8,8,1])] , train_y:[y_train[0],y_train[1]] , keep_prob:0.3})\n",
    "        test_data_fuc = get_test_data_trainfuc(test_data)\n",
    "        test_y_fuc = get_test_y_trainfuc(test_label)\n",
    "        print(sess.run(prediction,feed_dict={train_X:_data , train_y:_label , keep_prob:0.3}))\n",
    "        print(sess.run(accuracy,feed_dict={train_X:test_data_fuc , train_y:test_y_fuc , keep_prob:0.3}))\n",
    "            \n",
    "        true_y , pred_y = sess.run([y_true , y_pred],feed_dict={train_X:test_data_fuc , train_y:test_y_fuc })\n",
    "        p = precision_score(true_y,pred_y,average='binary')\n",
    "        r = recall_score(true_y,pred_y,average='binary')\n",
    "        f1score = f1_score(true_y,pred_y,average='binary')\n",
    "        print(p,r,f1score)\n",
    "        \n",
    "        gene_target=get_target_data_trainfuc(gene_test)\n",
    "        target = sess.run([y_pred],feed_dict={train_X:gene_target})\n",
    "        target=np.array(target).reshape((200))\n",
    "        target_csv = pd.DataFrame(columns=['result'],data=target)\n",
    "        target_csv.to_csv('./submission/submission'+str(f1score)+'.csv',index=False)\n",
    "        #print(W_conv1.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6318"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=list(gene_train[0:0])+list(gene_train[200:])\n",
    "len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61833423  0.38166577]\n",
      " [ 0.96418899  0.03581107]\n",
      " [ 0.8301571   0.16984287]\n",
      " [ 0.05285285  0.94714719]\n",
      " [ 0.9577775   0.0422225 ]\n",
      " [ 0.25869077  0.74130923]\n",
      " [ 0.58422863  0.41577134]\n",
      " [ 0.85390413  0.14609593]\n",
      " [ 0.58422863  0.41577134]\n",
      " [ 0.86701924  0.13298072]]\n",
      "0.6\n",
      "0.574712643678 0.537634408602 0.555555555556\n",
      "[[ 0.64776498  0.35223505]\n",
      " [ 0.56507289  0.43492705]\n",
      " [ 0.21601027  0.78398967]\n",
      " [ 0.15221652  0.84778345]\n",
      " [ 0.82405025  0.1759498 ]\n",
      " [ 0.64776498  0.35223505]\n",
      " [ 0.82229936  0.17770068]\n",
      " [ 0.89660645  0.10339357]\n",
      " [ 0.41002798  0.58997202]\n",
      " [ 0.41002798  0.58997202]]\n",
      "0.495\n",
      "0.535714285714 0.420560747664 0.471204188482\n",
      "[[ 0.53101701  0.46898308]\n",
      " [ 0.87463957  0.12536037]\n",
      " [ 0.86758596  0.13241406]\n",
      " [ 0.20450476  0.79549527]\n",
      " [ 0.96413809  0.03586193]\n",
      " [ 0.53101921  0.46898082]\n",
      " [ 0.86674869  0.13325129]\n",
      " [ 0.86767155  0.13232847]\n",
      " [ 0.19270524  0.80729479]\n",
      " [ 0.96414441  0.03585558]]\n",
      "0.575\n",
      "0.514851485149 0.590909090909 0.550264550265\n",
      "[[ 0.74565524  0.25434473]\n",
      " [ 0.94991177  0.05008829]\n",
      " [ 0.8467198   0.15328017]\n",
      " [ 0.7891596   0.21084036]\n",
      " [ 0.95244509  0.04755483]\n",
      " [ 0.82478398  0.17521608]\n",
      " [ 0.79881251  0.20118749]\n",
      " [ 0.95002538  0.04997462]\n",
      " [ 0.52144629  0.47855371]\n",
      " [ 0.78915954  0.21084049]]\n",
      "0.52\n",
      "0.495145631068 0.536842105263 0.515151515152\n",
      "[[ 0.36527178  0.63472819]\n",
      " [ 0.34806597  0.65193403]\n",
      " [ 0.06670891  0.93329108]\n",
      " [ 0.06670968  0.93329036]\n",
      " [ 0.66727495  0.33272508]\n",
      " [ 0.25460494  0.74539512]\n",
      " [ 0.93739855  0.06260142]\n",
      " [ 0.46306366  0.53693634]\n",
      " [ 0.06670891  0.93329108]\n",
      " [ 0.75807023  0.24192975]]\n",
      "0.51\n",
      "0.426829268293 0.406976744186 0.416666666667\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    test_data=list(gene_train[index*200:index*200+200])#pd.concat([train.ix[index*200:index*200+200]],ignore_index =True)\n",
    "    test_label=y_train[index*200:index*200+200]\n",
    "    train_data=list(gene_train[0:index*200])+list(gene_train[index*200+200:])#pd.concat([train[0:index*200],train[index*200+200:]],ignore_index=True )\n",
    "    train_label=y_train[0:index*200]+y_train[index*200+200:]\n",
    "    train_func(train_data,train_label,test_data,test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
