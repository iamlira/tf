{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.contrib.learn.python.learn.estimators.estimator import SKCompat\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.python.ops import array_ops as array_ops_\n",
    "learn = tf.contrib.learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd_ads=pd.read_csv('D:\\\\sale\\\\t_ads.csv')\n",
    "pd_comment=pd.read_csv('D:\\\\sale\\\\t_comment.csv')\n",
    "pd_order=pd.read_csv('D:\\\\sale\\\\t_order.csv')\n",
    "pd_product=pd.read_csv('D:\\\\sale\\\\t_product.csv')\n",
    "pd_sale=pd.read_csv('D:\\\\sale\\\\t_sales_sum.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sale_1036=pd_sale[pd_sale['shop_id']==1036]\n",
    "comment_1036=pd_comment[pd_comment['shop_id']==1036]\n",
    "order_1036=pd_order[pd_order['shop_id']==1036]\n",
    "product_1036=pd_product[pd_product['shop_id']==1036]\n",
    "ads_1036=pd_ads[pd_ads['shop_id']==1036]\n",
    "\n",
    "sale_1036['dt']=pd.to_datetime(sale_1036['dt'])\n",
    "comment_1036['create_dt']=pd.to_datetime(comment_1036['create_dt'])\n",
    "order_1036['ord_dt']=pd.to_datetime(order_1036['ord_dt'])\n",
    "product_1036['on_dt']=pd.to_datetime(product_1036['on_dt'])\n",
    "ads_1036['create_dt']=pd.to_datetime(ads_1036['create_dt'])\n",
    "\n",
    "sale_1036=sale_1036.sort_values(['dt'],ascending=True)\n",
    "comment_1036=comment_1036.sort_values(['create_dt'],ascending=True)\n",
    "order_1036=order_1036.sort_values(['ord_dt'],ascending=True)\n",
    "product_1036=product_1036.sort_values(['on_dt'],ascending=True)\n",
    "ads_1036=ads_1036.sort_values(['create_dt'],ascending=True)\n",
    "\n",
    "sale_1036.to_csv('D:\\\\sale\\\\sale_1036.csv')\n",
    "comment_1036.to_csv('D:\\\\sale\\\\comment_1036.csv')\n",
    "order_1036.to_csv('D:\\\\sale\\\\order_1036.csv')\n",
    "product_1036.to_csv('D:\\\\sale\\\\product_1036.csv')\n",
    "ads_1036.to_csv('D:\\\\sale\\\\ads_1036.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shop_id=pd_sale['shop_id']\n",
    "shop_id=list(set(shop_id))\n",
    "sale_list=[]\n",
    "comment_list=[]\n",
    "order_list=[]\n",
    "product_list=[]\n",
    "ads_list=[]\n",
    "sale_list=[]\n",
    "\n",
    "for i in shop_id:\n",
    "    sale_list.append(pd_sale[pd_sale['shop_id']==i])\n",
    "    comment_list.append(pd_comment[pd_comment['shop_id']==i])\n",
    "    order_list.append(pd_order[pd_order['shop_id']==i])\n",
    "    product_list.append(pd_product[pd_product['shop_id']==i])\n",
    "    ads_list.append(pd_ads[pd_ads['shop_id']==i])\n",
    "    \n",
    "    sale_list[i-1]=sale_list[i-1].sort_values(['dt'],ascending=True)\n",
    "    comment_list[i-1]=comment_list[i-1].sort_values(['create_dt'],ascending=True)\n",
    "    order_list[i-1]=order_list[i-1].sort_values(['ord_dt'],ascending=True)\n",
    "    product_list[i-1]=product_list[i-1].sort_values(['on_dt'],ascending=True)\n",
    "    ads_list[i-1]=ads_list[i-1].sort_values(['create_dt'],ascending=True)\n",
    "    \n",
    "    sale_list[i-1]['dt']=pd.to_datetime(sale_list[i-1]['dt'])\n",
    "    comment_list[i-1]['create_dt']=pd.to_datetime(comment_list[i-1]['create_dt'])\n",
    "    order_list[i-1]['ord_dt']=pd.to_datetime(order_list[i-1]['ord_dt'])\n",
    "    product_list[i-1]['on_dt']=pd.to_datetime(product_list[i-1]['on_dt'])\n",
    "    ads_list[i-1]['create_dt']=pd.to_datetime(ads_list[i-1]['create_dt'])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(order_list)):\n",
    "    order_list[i].drop('pid',axis=1,inplace=True)\n",
    "    order_list[i].drop('shop_id',axis=1,inplace=True)\n",
    "    order_list[i]=order_list[i].groupby(['ord_dt']).sum()\n",
    "    \n",
    "for i in range(len(comment_list)):\n",
    "    comment_list[i].set_index(\"create_dt\", inplace=True)\n",
    "    comment_list[i].drop('shop_id',axis=1,inplace=True)\n",
    "for i in range(len(ads_list)):\n",
    "    ads_list[i].set_index(\"create_dt\",inplace=True)\n",
    "    ads_list[i].drop('shop_id',axis=1,inplace=True)\n",
    "for i in range(len(sale_list)):\n",
    "    sale_list[i].set_index(\"dt\",inplace=True)\n",
    "    sale_list[i].drop('shop_id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_list=[]\n",
    "for i in shop_id:\n",
    "    tmp=pd.DataFrame(index=pd.date_range('2016-8-1','2017-4-30'))\n",
    "    date_list.append(pd.concat([tmp,comment_list[i-1], order_list[i-1],ads_list[i-1]], axis=1, join_axes=[tmp.index]))\n",
    "    date_list[i-1].fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "order_1036.drop('pid',axis=1,inplace=True)\n",
    "order_1036=order_1036.groupby(['ord_dt']).sum()\n",
    "\n",
    "comment_1036.drop('shop_id',axis=1,inplace=True)\n",
    "order_1036.drop('shop_id',axis=1,inplace=True)\n",
    "ads_1036.drop('shop_id',axis=1,inplace=True)\n",
    "sale_1036.drop('shop_id',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "#order_1036.set_index(\"ord_dt\", inplace=True) order_1036已将ord_dt作为索引\n",
    "sale_1036.set_index(\"dt\", inplace=True)\n",
    "comment_1036.set_index(\"create_dt\", inplace=True)\n",
    "product_1036.set_index(\"on_dt\", inplace=True)\n",
    "ads_1036.set_index(\"create_dt\", inplace=True)\n",
    "\n",
    "sale_1036"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#date_list=pd.date_range('2016-6-1','2017-4-30')\n",
    "date=pd.DataFrame(index=pd.date_range('2016-8-1','2017-4-30'))\n",
    "data=pd.concat([date,comment_1036, order_1036,ads_1036], axis=1, join_axes=[date.index])\n",
    "data.fillna(value=0,inplace=True)\n",
    "data.to_csv('D:\\\\sale\\\\data_1036.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(df_list):\n",
    "    for i in range(len(df_list)):\n",
    "        for j in [col for col in df_list[i]]:\n",
    "            t_max=df_list[i][j].max()\n",
    "            t_min=df_list[i][j].min()\n",
    "            down = 1 if (t_max-t_min == 0) else (t_max-t_min)\n",
    "            df_list[i][j]=(df_list[i][j]-t_min)/down\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data.to_period('M')\n",
    "data.resample('M').sum()\n",
    "for i in range(len(date_list)):\n",
    "    date_list[i]=date_list[i].resample('M').sum()\n",
    "    \n",
    "date_list[0].shape\n",
    "print(np.array(date_list[0],dtype=np.float32))\n",
    "\n",
    "norm(date_list)\n",
    "\n",
    "for i in range(len(sale_list)):\n",
    "    for j in [col for col in sale_list[i]]:\n",
    "        sale_list[i][j]=pow(sale_list[i][j],0.5)\n",
    "#norm(sale_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 30\n",
    "NUM_LAYERS = 2\n",
    "\n",
    "TIMESTEPS = 30\n",
    "TRAINING_STEPS = 3000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TRAINING_EXAMPLES = 10000\n",
    "TESTING_EXAMPLES = 1000\n",
    "SAMPLE_GAP = 0.01\n",
    "\n",
    "timesteps=30\n",
    "num_input=14\n",
    "num_hidden=18\n",
    "#learning_rate= 0.01\n",
    "training_steps=70000\n",
    "\n",
    "LEARNING_RATE_BASE = 0.00001\n",
    "LEARNING_RATE_DECAY = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_train_data(data,labels):\n",
    "    X = []\n",
    "    y = []\n",
    "    seq=[0,31,61,92,122]\n",
    "\n",
    "    for j in range(len(sale_list)):\n",
    "        for i in range(5):\n",
    "            X.append(np.array(data[j][seq[i]: seq[i] + 30],dtype=np.float32))\n",
    "            y.append(np.array(labels[j].iloc[i + 2],dtype=np.float32))\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "def generate_valid_data(data,labels):\n",
    "    X = []\n",
    "    y = []\n",
    "    seq=[92,122]\n",
    "\n",
    "    for j in range(len(sale_list)):\n",
    "        for i in range(2):\n",
    "            X.append(np.array(data[j][seq[i]: seq[i] + 30],dtype=np.float32))\n",
    "            y.append(np.array(labels[j].iloc[i + 2],dtype=np.float32))\n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X,train_y=generate_train_data(date_list,sale_list)\n",
    "#valid_X,valid_y=generate_valid_data(date_list,sale_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None,1])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    'out': tf.Variable(tf.random_normal([num_hidden, 1]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([1]))\n",
    "}\n",
    "global_step = tf.Variable(0, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"rnn\",reuse=tf.AUTO_REUSE):\n",
    "    logits = RNN(X, weights, biases)\n",
    "    #prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    #loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    #    logits=logits, labels=Y))\n",
    "    #loss_op=tf.reduce_mean(tf.square(tf.reshape(logits,[-1])-tf.reshape(Y, [-1])))\n",
    "    loss_op = tf.reduce_mean(tf.square(logits - Y))\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "            LEARNING_RATE_BASE,\n",
    "            global_step,\n",
    "            2000, LEARNING_RATE_DECAY,\n",
    "            staircase=True)\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    #train_op = optimizer.minimize(loss_op,global_step=global_step)\n",
    "    train_op = tf.contrib.layers.optimize_loss(\n",
    "            loss_op,\n",
    "            tf.contrib.framework.get_global_step(),\n",
    "            optimizer='Adam',\n",
    "            learning_rate=learning_rate)\n",
    "    \n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    #correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "    correct_pred = tf.sqrt(tf.square(logits - Y))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "    \n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"D:\\\\log\\\\jdd.log\", tf.get_default_graph())\n",
    "tf.summary.scalar(\"loss\",loss_op)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "    batch=0\n",
    "    test_X=[]\n",
    "    test_y=[]\n",
    "    for i in range(len(date_list)):\n",
    "        test_X.append(np.array(date_list[i][153:183],dtype=np.float32))\n",
    "        test_y.append(np.array(sale_list[i].iloc[-1],dtype=np.float32))\n",
    "    for step in range(1, training_steps+1):\n",
    "        #batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        #batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        # Run optimization op (backprop)\n",
    "        _,summary=sess.run([train_op,merged], feed_dict={X: train_X[batch:batch+10], Y: train_y[batch:batch+10]})\n",
    "        #loss=sess.run(loss_op,feed_dict={X:valid_X[batch:batch+10],Y:valid_y[batch:batch+10]})\n",
    "        writer.add_summary(summary, step)\n",
    "        batch+=1\n",
    "        if batch==len(date_list):\n",
    "            batch=0\n",
    "        if step%500==0:\n",
    "            # 配置运行时需要记录的信息。\n",
    "            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "            # 运行时记录运行信息的proto。\n",
    "            run_metadata = tf.RunMetadata()\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss = sess.run(loss_op, feed_dict={X: test_X,Y: test_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) )\n",
    "#            pre,rate=sess.run([logits,learning_rate],feed_dict={X:test_X,Y:test_y})\n",
    "            writer.add_run_metadata(run_metadata=run_metadata, tag=(\"tag%d\" % step), global_step=step)\n",
    "#            print(\"rate \"+ str(rate))\n",
    "#            file=open('D:\\\\result.txt','w+')\n",
    "#            result=str(pre)\n",
    "#            file.writelines(result)\n",
    "#            file.close()\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    #test_len = 128\n",
    "    #test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    #test_label = mnist.test.labels[:test_len]\n",
    "    #print(\"Testing Accuracy:\", \\\n",
    "    #    sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n",
    "    \n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_X, Y: test_y}))\n",
    "    pre=sess.run(logits,feed_dict={X:test_X,Y:test_y})\n",
    "    predict_X=[]\n",
    "    for i in range(len(date_list)):\n",
    "        predict_X.append(np.array(date_list[i].iloc[243:273]))\n",
    "    predict=sess.run(logits,feed_dict={X:predict_X})\n",
    "    file=open('D:\\\\result.txt','w+')\n",
    "    result=str(pre)\n",
    "    file.writelines(result)\n",
    "    file.close()\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
